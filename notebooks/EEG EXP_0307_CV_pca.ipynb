{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66e2a4c-39dd-4bd6-9c49-e4d04ba16c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4914a1ce-95de-429a-af7d-280a39e08112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "from pyro.contrib.gp.kernels.kernel import Kernel\n",
    "from pyro.nn.module import PyroParam\n",
    "\n",
    "\n",
    "def _torch_sqrt(x, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A convenient function to avoid the NaN gradient issue of :func:`torch.sqrt`\n",
    "    at 0.\n",
    "    \"\"\"\n",
    "    # Ref: https://github.com/pytorch/pytorch/issues/2421\n",
    "    return (x + eps).sqrt()\n",
    "\n",
    "\n",
    "class Isotropy(Kernel):\n",
    "    \"\"\"\n",
    "    Base class for a family of isotropic covariance kernels which are functions of the\n",
    "    distance :math:`|x-z|*l`, where :math:`l` is the length-scale parameter.\n",
    "\n",
    "    By default, the parameter ``lengthscale`` has size 1. To use the isotropic version\n",
    "    (different lengthscale for each dimension), make sure that ``lengthscale`` has size\n",
    "    equal to ``input_dim``.\n",
    "\n",
    "    :param torch.Tensor lengthscale: Length-scale parameter of this kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n",
    "        super().__init__(input_dim, active_dims)\n",
    "\n",
    "        variance = torch.tensor(1.0) if variance is None else variance\n",
    "        self.variance = PyroParam(variance, constraints.positive)\n",
    "\n",
    "        lengthscale = torch.tensor(1.0) if lengthscale is None else lengthscale\n",
    "        self.lengthscale = PyroParam(lengthscale, constraints.positive)\n",
    "\n",
    "    def _square_scaled_dist(self, X, Z=None):\n",
    "        r\"\"\"\n",
    "        Returns :math:`\\|\\frac{X-Z}{l}\\|^2`.\n",
    "        \"\"\"\n",
    "        if Z is None:\n",
    "            Z = X\n",
    "        X = self._slice_input(X)\n",
    "        Z = self._slice_input(Z)\n",
    "        if X.size(1) != Z.size(1):\n",
    "            raise ValueError(\"Inputs must have the same number of features.\")\n",
    "\n",
    "        scaled_X = X * self.lengthscale\n",
    "        scaled_Z = Z * self.lengthscale\n",
    "        X2 = (scaled_X ** 2).sum(1, keepdim=True)\n",
    "        Z2 = (scaled_Z ** 2).sum(1, keepdim=True)\n",
    "        XZ = scaled_X.matmul(scaled_Z.t())\n",
    "        r2 = X2 - 2 * XZ + Z2.t()\n",
    "        return r2.clamp(min=0)\n",
    "\n",
    "    def _scaled_dist(self, X, Z=None):\n",
    "        r\"\"\"\n",
    "        Returns :math:`l * \\|X-Z\\|`.\n",
    "        \"\"\"\n",
    "        return _torch_sqrt(self._square_scaled_dist(X, Z))\n",
    "\n",
    "    def _diag(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the diagonal part of covariance matrix on active features.\n",
    "        \"\"\"\n",
    "        return self.variance.expand(X.size(0))\n",
    "\n",
    "\n",
    "class INV_RBF(Isotropy):\n",
    "    r\"\"\"\n",
    "    Implementation of Radial Basis Function kernel:\n",
    "\n",
    "        :math:`k(x,z) = \\sigma^2\\exp\\left(-0.5 \\times \\frac{|x-z|^2}{l^2}\\right).`\n",
    "\n",
    "    .. note:: This kernel also has name `Squared Exponential` in literature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n",
    "        super().__init__(input_dim, variance, lengthscale, active_dims)\n",
    "\n",
    "    def forward(self, X, Z=None, diag=False):\n",
    "        if diag:\n",
    "            return self._diag(X)\n",
    "\n",
    "        r2 = self._square_scaled_dist(X, Z)\n",
    "        return self.variance * torch.exp(-0.5 * r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2532a4-e54b-4e1f-bf41-4b17d36fec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, brier_score_loss\n",
    "\n",
    "def Predict_GPC(GPC, X_test,y_test):\n",
    "    with torch.no_grad():\n",
    "        f_loc, f_scale = GPC(X_test)\n",
    "        p_test = GPC.likelihood.response_function(f_loc).cpu().numpy().astype(\"float64\")\n",
    "    print(\" Test sample size: \", len(p_test))\n",
    "    nll = log_loss(y_test.cpu(), p_test) #negative log likelihood\n",
    "    auc = roc_auc_score(y_test.cpu(), p_test) #AUC\n",
    "    brier_score = brier_score_loss(y_test.cpu(), p_test) #brier_score_loss\n",
    "    y_pred = np.zeros(len(p_test))\n",
    "    y_pred[p_test>= 0.5] = 1\n",
    "    accuracy = np.sum(y_test.cpu().numpy() == y_pred)/len(y_test.cpu().numpy())\n",
    "    print(\" accuracy: {} \\n negative log likelihood: {} \\n brier_score_loss:{} \\n AUC: {}\".format(accuracy, nll, brier_score, auc))\n",
    "    return np.array([accuracy, nll, brier_score, auc]),p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328e5ee1-845d-4f24-bd01-a9d12f99fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.load('EEG_X.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f1baac-911c-44af-b828-385a6d826ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.load('EEG_y.npy')\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac5d822-0c8b-4187-802c-862ca8bdcef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X[0][0])\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57e3717-eab6-44d1-8e6b-78a69529c1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 57, 122)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdfb2ca-1a52-42bb-a59c-ffd395e19153",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6987ec7-1ad1-47d8-9dfc-fb9200b33967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 0\n",
      "0th time\n",
      "30th time\n",
      "60th time\n",
      "90th time\n",
      "120th time\n",
      "150th time\n",
      "180th time\n",
      "210th time\n",
      "240th time\n",
      "Cross Validation: 1\n",
      "0th time\n",
      "30th time\n",
      "60th time\n",
      "90th time\n",
      "120th time\n",
      "150th time\n",
      "180th time\n",
      "210th time\n",
      "240th time\n",
      "Cross Validation: 2\n",
      "0th time\n",
      "30th time\n",
      "60th time\n",
      "90th time\n",
      "120th time\n",
      "150th time\n",
      "180th time\n",
      "210th time\n",
      "240th time\n",
      "Cross Validation: 3\n",
      "0th time\n",
      "30th time\n",
      "60th time\n",
      "90th time\n",
      "120th time\n",
      "150th time\n",
      "180th time\n",
      "210th time\n",
      "240th time\n",
      "Cross Validation: 4\n",
      "0th time\n",
      "30th time\n",
      "60th time\n",
      "90th time\n",
      "120th time\n",
      "150th time\n",
      "180th time\n",
      "210th time\n",
      "240th time\n",
      "CPU times: user 1d 4h 7min 10s, sys: 23min 15s, total: 1d 4h 30min 26s\n",
      "Wall time: 1d 3h 54min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "L_cv = []\n",
    "res_cv = []\n",
    "y_cv = []\n",
    "cv = 0\n",
    "for ind_train, ind_test in kf.split(X=np.zeros([122,57]), y=y):\n",
    "    print('Cross Validation:',cv)\n",
    "    cv+=1\n",
    "    L = []\n",
    "    L_exp = []\n",
    "    y_predict = []\n",
    "    np.random.seed(0)\n",
    "    pyro.set_rng_seed(0)\n",
    "    t = 0\n",
    "    for data in X:\n",
    "        L_0 = []\n",
    "        if t%30==0:\n",
    "            print(\"{}th time\".format(t))\n",
    "        t+=1\n",
    "        data = np.transpose(data)\n",
    "        X_train, y_train = data[ind_train], y[ind_train]\n",
    "        X_test, y_test = data[ind_test], y[ind_test]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca_x = pca.fit_transform(X_train)[:,-20:]\n",
    "        pca_x = scaler.fit_transform(pca_x)\n",
    "\n",
    "    #     X_train = torch.tensor(X_train,dtype= torch.float32)\n",
    "    #     y_train = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
    "    #     X_test = torch.tensor(X_test,dtype= torch.float32)\n",
    "    #     y_test = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
    "\n",
    "        # Choose kernel and likelihood for GP\n",
    "        for random_seed in range(20):\n",
    "    #         np.random.seed(random_seed)\n",
    "            pyro.set_rng_seed(random_seed)\n",
    "            N = X_train.shape[0]\n",
    "            X_ = np.insert(X_train,0,pca_x[:,random_seed],axis=1)\n",
    "            p = X_.shape[1]\n",
    "            X_ = torch.tensor(X_)\n",
    "            y_samples = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
    "            kernel = INV_RBF(input_dim = p, variance = torch.tensor(1.),lengthscale = torch.ones(p))\n",
    "            likelihood = gp.likelihoods.Binary()\n",
    "            gpc = gp.models.VariationalGP(X_,y_samples,kernel=kernel,jitter = torch.tensor(1e-06), \n",
    "                                        likelihood=likelihood,\n",
    "                                        whiten =True)\n",
    "            gpc.kernel.lengthscale =  pyro.nn.PyroSample(dist.Exponential(torch.tensor([2.0])).expand([p]).to_event(1))\n",
    "            gpc.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(0.0), torch.tensor(2.0)))\n",
    "            num_steps = 1000\n",
    "            initial_lr = 0.005\n",
    "            gamma = 0.5  # final learning rate will be gamma * initial_lr\n",
    "            lrd = gamma ** (1 / num_steps)\n",
    "            optim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "\n",
    "            svi = SVI(gpc.model,gpc.guide,optim,loss=Trace_ELBO())\n",
    "            losses =np.zeros(num_steps)\n",
    "\n",
    "            pyro.clear_param_store()\n",
    "            for iteration in range(num_steps):\n",
    "                losses[iteration]=svi.step()\n",
    "\n",
    "            L_0.append(pyro.param('kernel.lengthscale_map').data.cpu().numpy())\n",
    "\n",
    "        L.append(L_0)\n",
    "        L_0 = np.array(L_0)\n",
    "        threshold = np.quantile(L_0[:,0],0.6)\n",
    "\n",
    "        selected_var = np.quantile(L_0[:,1:],0.5,axis = 0)>=threshold\n",
    "        L_exp.append(selected_var)\n",
    "\n",
    "        if sum(selected_var)==0:\n",
    "            max_ind = np.argmax(np.quantile(L_0[:,1:],0.5,axis = 0))\n",
    "            selected_var[max_ind]=True\n",
    "\n",
    "        X_selected = X_train[:,selected_var]\n",
    "        X_test = X_test[:,selected_var]\n",
    "        kernel = 1.0 * RBF(np.ones(X_selected.shape[1]))\n",
    "        gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "                 random_state=0).fit(X_selected, y_train)\n",
    "        y_predict.append(gpc.predict(X_test))\n",
    "    \n",
    "    L_cv.append(L)\n",
    "    y_predict = np.array(y_predict)\n",
    "    res = []\n",
    "    for ind in range(y_predict.shape[1]):\n",
    "        a = y_predict[:,ind]\n",
    "        lst = []\n",
    "        for n,c in groupby(a):\n",
    "\n",
    "            num,count = n,sum(1 for i in c)\n",
    "            lst.append([num,count])\n",
    "\n",
    "        lst = np.array(lst)\n",
    "        res.append(lst[np.argmax(lst[:,1])][0])\n",
    "    res_cv.append(res)\n",
    "    y_cv.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3d58c6-2dd8-41f3-bcb4-3e2dd796f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv = np.array(y_cv)\n",
    "L_cv = np.array(L_cv)\n",
    "res_cv = np.array(res_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1876f4-11b9-4976-9d37-af9efb64e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"L_cv_pca_0307.npy\", L_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8431860-de5c-4d25-a310-d0a2780fb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_cv_pca_0307.npy\", y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb6db66-5c9f-4a42-a9cf-c7984bd49f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"res_cv_pca_0307.npy\", res_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789aaa18-6f95-4cb5-a634-06c7fc52d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 0\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 1\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 2\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 3\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 4\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "CPU times: user 5min 53s, sys: 1min 13s, total: 7min 6s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = 0\n",
    "res_cv = []\n",
    "for ind_train, ind_test in kf.split(X=np.zeros([122,57]), y=y):\n",
    "    print('Cross Validation:',cv)\n",
    "    \n",
    "    y_predict = []\n",
    "    np.random.seed(0)\n",
    "    pyro.set_rng_seed(0)\n",
    "    t = 0\n",
    "    for data in X:\n",
    "        L_0 = []\n",
    "        if t%50==0:\n",
    "            print(\"{}th time\".format(t))\n",
    "        \n",
    "        data = np.transpose(data)\n",
    "        X_train, y_train = data[ind_train], y[ind_train]\n",
    "        X_test, y_test = data[ind_test], y[ind_test]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        L_0 = np.array(L_cv[cv][t])\n",
    "        threshold = np.quantile(L_0[:,0],0.6)\n",
    "\n",
    "        selected_var = np.quantile(L_0[:,1:],0.5,axis = 0)>=threshold\n",
    "\n",
    "        if sum(selected_var)==0:\n",
    "            max_ind = np.argmax(np.quantile(L_0[:,1:],0.5,axis = 0))\n",
    "            selected_var[max_ind]=True\n",
    "\n",
    "        X_selected = X_train[:,selected_var]\n",
    "        X_test = X_test[:,selected_var]\n",
    "        kernel = 1.0 * RBF(np.ones(X_selected.shape[1]))\n",
    "        gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "                 random_state=0).fit(X_selected, y_train)\n",
    "        y_predict.append(gpc.predict(X_test))\n",
    "        \n",
    "        t+=1\n",
    "        \n",
    "    y_predict = np.array(y_predict)\n",
    "    res = []\n",
    "    for ind in range(y_predict.shape[1]):\n",
    "        a = y_predict[:,ind]\n",
    "        lst = []\n",
    "        for n,c in groupby(a):\n",
    "\n",
    "            num,count = n,sum(1 for i in c)\n",
    "            lst.append([num,count])\n",
    "\n",
    "        lst = np.array(lst)\n",
    "        res.append(lst[np.argmax(lst[:,1])][0])\n",
    "    res_cv.append(res)\n",
    "    cv+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a97df6d8-cac8-4144-b358-aeab9e534a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "selected_acc = []\n",
    "selected_f1 = []\n",
    "selected_tpr = []\n",
    "selected_fpr = []\n",
    "\n",
    "for j in range(len(res_cv)):\n",
    "    selected_acc.append(accuracy_score(y_cv[j], res_cv[j]))\n",
    "    selected_f1.append(f1_score(y_cv[j], res_cv[j], average='weighted'))\n",
    "    tpr = sum((np.array(y_cv[j]).squeeze()+np.array(res_cv[j])) == 2)/sum(np.array(y_cv[j]).squeeze())\n",
    "    selected_tpr.append(tpr)\n",
    "    fpr = sum((np.array(y_cv[j]).squeeze()-np.array(res_cv[j])) == -1)/(len(np.array(y_cv[j]).squeeze()) - sum(np.array(y_cv[j]).squeeze()))\n",
    "    selected_fpr.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70220498-9285-4056-b6e9-7c04b7989593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.704, 0.15297276446043154)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(selected_acc), np.std(selected_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ecb0f-d681-4ded-a651-a1fd4f8c1d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86c5c13-b798-4a30-ac0c-3d76d9a53b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 0\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 1\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 2\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 3\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 4\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "CPU times: user 8min 58s, sys: 1min 52s, total: 10min 50s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = 0\n",
    "res_cv = []\n",
    "for ind_train, ind_test in kf.split(X=np.zeros([122,57]), y=y):\n",
    "    print('Cross Validation:',cv)\n",
    "    \n",
    "    y_predict = []\n",
    "    np.random.seed(0)\n",
    "    pyro.set_rng_seed(0)\n",
    "    t = 0\n",
    "    for data in X:\n",
    "        L_0 = []\n",
    "        if t%50==0:\n",
    "            print(\"{}th time\".format(t))\n",
    "        \n",
    "        data = np.transpose(data)\n",
    "        X_train, y_train = data[ind_train], y[ind_train]\n",
    "        X_test, y_test = data[ind_test], y[ind_test]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        L_0 = np.array(L_cv[cv][t])\n",
    "        threshold = np.quantile(L_0[:,0],0.55)\n",
    "\n",
    "        selected_var = np.quantile(L_0[:,1:],0.5,axis = 0)>=threshold\n",
    "\n",
    "        if sum(selected_var)==0:\n",
    "            max_ind = np.argmax(np.quantile(L_0[:,1:],0.5,axis = 0))\n",
    "            selected_var[max_ind]=True\n",
    "\n",
    "        X_selected = X_train[:,selected_var]\n",
    "        X_test = X_test[:,selected_var]\n",
    "        kernel = 1.0 * RBF(np.ones(X_selected.shape[1]))\n",
    "        gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "                 random_state=0).fit(X_selected, y_train)\n",
    "        y_predict.append(gpc.predict(X_test))\n",
    "        \n",
    "        t+=1\n",
    "        \n",
    "    y_predict = np.array(y_predict)\n",
    "    res = []\n",
    "    for ind in range(y_predict.shape[1]):\n",
    "        a = y_predict[:,ind]\n",
    "        lst = []\n",
    "        for n,c in groupby(a):\n",
    "\n",
    "            num,count = n,sum(1 for i in c)\n",
    "            lst.append([num,count])\n",
    "\n",
    "        lst = np.array(lst)\n",
    "        res.append(lst[np.argmax(lst[:,1])][0])\n",
    "    res_cv.append(res)\n",
    "    cv+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4313c18-b0b9-4ed2-9997-13fb2281dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "selected_acc = []\n",
    "selected_f1 = []\n",
    "selected_tpr = []\n",
    "selected_fpr = []\n",
    "\n",
    "for j in range(len(res_cv)):\n",
    "    selected_acc.append(accuracy_score(y_cv[j], res_cv[j]))\n",
    "    selected_f1.append(f1_score(y_cv[j], res_cv[j], average='weighted'))\n",
    "    tpr = sum((np.array(y_cv[j]).squeeze()+np.array(res_cv[j])) == 2)/sum(np.array(y_cv[j]).squeeze())\n",
    "    selected_tpr.append(tpr)\n",
    "    fpr = sum((np.array(y_cv[j]).squeeze()-np.array(res_cv[j])) == -1)/(len(np.array(y_cv[j]).squeeze()) - sum(np.array(y_cv[j]).squeeze()))\n",
    "    selected_fpr.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e968cd9d-19ad-4bc9-a0fd-26356c6b3b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7366666666666667, 0.16639644761165362)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(selected_acc), np.std(selected_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016dee7-1a0e-43ac-ae35-8240a2141954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "505f09de-a4af-4d16-b79e-31b08b0fc37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 0\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 1\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 2\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 3\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "Cross Validation: 4\n",
      "0th time\n",
      "50th time\n",
      "100th time\n",
      "150th time\n",
      "200th time\n",
      "250th time\n",
      "CPU times: user 13min 55s, sys: 2min 48s, total: 16min 43s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = 0\n",
    "res_cv = []\n",
    "for ind_train, ind_test in kf.split(X=np.zeros([122,57]), y=y):\n",
    "    print('Cross Validation:',cv)\n",
    "    \n",
    "    y_predict = []\n",
    "    np.random.seed(0)\n",
    "    pyro.set_rng_seed(0)\n",
    "    t = 0\n",
    "    for data in X:\n",
    "        L_0 = []\n",
    "        if t%50==0:\n",
    "            print(\"{}th time\".format(t))\n",
    "        \n",
    "        data = np.transpose(data)\n",
    "        X_train, y_train = data[ind_train], y[ind_train]\n",
    "        X_test, y_test = data[ind_test], y[ind_test]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        L_0 = np.array(L_cv[cv][t])\n",
    "        threshold = np.quantile(L_0[:,0],0.5)\n",
    "\n",
    "        selected_var = np.quantile(L_0[:,1:],0.5,axis = 0)>=threshold\n",
    "\n",
    "        if sum(selected_var)==0:\n",
    "            max_ind = np.argmax(np.quantile(L_0[:,1:],0.5,axis = 0))\n",
    "            selected_var[max_ind]=True\n",
    "\n",
    "        X_selected = X_train[:,selected_var]\n",
    "        X_test = X_test[:,selected_var]\n",
    "        kernel = 1.0 * RBF(np.ones(X_selected.shape[1]))\n",
    "        gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "                 random_state=0).fit(X_selected, y_train)\n",
    "        y_predict.append(gpc.predict(X_test))\n",
    "        \n",
    "        t+=1\n",
    "        \n",
    "    y_predict = np.array(y_predict)\n",
    "    res = []\n",
    "    for ind in range(y_predict.shape[1]):\n",
    "        a = y_predict[:,ind]\n",
    "        lst = []\n",
    "        for n,c in groupby(a):\n",
    "\n",
    "            num,count = n,sum(1 for i in c)\n",
    "            lst.append([num,count])\n",
    "\n",
    "        lst = np.array(lst)\n",
    "        res.append(lst[np.argmax(lst[:,1])][0])\n",
    "    res_cv.append(res)\n",
    "    cv+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3cca857-8c62-401c-8fe1-df854a23da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "selected_acc = []\n",
    "selected_f1 = []\n",
    "selected_tpr = []\n",
    "selected_fpr = []\n",
    "\n",
    "for j in range(len(res_cv)):\n",
    "    selected_acc.append(accuracy_score(y_cv[j], res_cv[j]))\n",
    "    selected_f1.append(f1_score(y_cv[j], res_cv[j], average='weighted'))\n",
    "    tpr = sum((np.array(y_cv[j]).squeeze()+np.array(res_cv[j])) == 2)/sum(np.array(y_cv[j]).squeeze())\n",
    "    selected_tpr.append(tpr)\n",
    "    fpr = sum((np.array(y_cv[j]).squeeze()-np.array(res_cv[j])) == -1)/(len(np.array(y_cv[j]).squeeze()) - sum(np.array(y_cv[j]).squeeze()))\n",
    "    selected_fpr.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e233d6d6-7e0a-43cc-acbc-10d2e965900d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7453333333333333, 0.14327673301070984)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(selected_acc), np.std(selected_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
